{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok, então a classe para nome curto é na tag 'a' com 'class' = 'link-gray'\n",
    "crude_hacker_name = soup.find_all('a', attrs={'class' : 'link-gray'})\n",
    "\n",
    "#One problem is that, there is other classes that have 'link-gray' in it, and needs to be removed from crude_hacker_name \n",
    "tofilter = soup.find_all('a',attrs={'class':\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\"})\n",
    "tofilter = tofilter + soup.find_all('a',attrs={'class':\"py-2 pb-0 lh-condensed-ultra d-block link-gray no-underline f5\"})\n",
    "                                            \n",
    "#if our crude_hacker_name is in this tofilter list, it is not saved\n",
    "clean_hacker_names = [name for name in crude_hacker_name if name not in tofilter]\n",
    "\n",
    "#and now we select only the text:\n",
    "hacker_names = [name.text.strip() for name in clean_hacker_names]\n",
    "\n",
    "#testing\n",
    "print(hacker_names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the complete name is <h1 class=\"h3 lh-condensed\">\n",
    "crude_full_names = soup.find_all('h1', attrs={'class' : \"h3 lh-condensed\"})\n",
    "full_names = [full_name.text.strip() for full_name in crude_full_names]\n",
    "\n",
    "#testing\n",
    "print(full_names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you notice, there are some words in the end of the list, that are not related to programer names:\n",
    "print(hacker_names[-4:])\n",
    "\n",
    "#notice that after pairing the hacker_names with the full names, those words wont be carried on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW THE ANSWER:\n",
    "list_of_names = [f'{hacker_names[i]} ({full_names[i]})' for i in range(len(full_names))]\n",
    "list_of_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first make the soup\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok, one interesting thing is that, in github, the link to the repository has \"name_of_person\"/\"name_of_repository\"\n",
    "\n",
    "#all the repositories are in tag 'h1', since they are kind of \"headers\" of cells\n",
    "\n",
    "repos_crude = soup.find_all('h1')\n",
    "\n",
    "#now, every \"h1\" has a subclass 'a' where the \"href\" is the name of the repository:\n",
    "person_and_repos = [repo.find_all('a')[0]['href'] for repo in repos_crude if (len(repo.find_all('a')) != 0)]\n",
    "\n",
    "#we now print the repositores\n",
    "\n",
    "print('The trending repositories are:\\b')\n",
    "for text in person_and_repos:\n",
    "    a = text.split('/')[2]\n",
    "    b = text.split('/')[1]\n",
    "    print(f'repository: {a} | creator: {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPPOSE WE ONLY WANT THE IMAGES/PHOTOGRAPHS, to the tiny images that appear on the bottom, like country flags, stars, etc\n",
    "\n",
    "#all images are inside a tag div and a class called \"thumbinner\", with exception of the first one\n",
    "images_thumbinner = soup.find_all('div',attrs={'class':\"thumbinner\"})\n",
    "\n",
    "#image links are stored in 'src', in the html text\n",
    "images = ['https:' + thumbinner.find_all('img')[0]['src'] for thumbinner in images_thumbinner]\n",
    "\n",
    "#However, there is some other images, which are inside a Table tag: <table class=\"infobox biography vcard\"...\n",
    "#those are the images of walt disney's portrait and his signature\n",
    "html_chuncks = soup.find_all('table')\n",
    "table_images = [html_chunck.find_all('a',attrs={'class':'image'})[0] for html_chunck in html_chuncks\\\n",
    "                                    if (len(html_chunck.find_all('a',attrs={'class':'image'})) != 0)]\n",
    "\n",
    "#and now we apply again:\n",
    "images_2 = ['https:' + html_chunck.find_all('img')[0]['src'] for html_chunck in table_images]\n",
    "images_2\n",
    "\n",
    "#now we join everything:\n",
    "images = images_2+images\n",
    "print('important image links are:\\n')\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will need a function\n",
    "def try_href(html_chunk):\n",
    "    '''This function returns the href class of a html chunck, if exists '''\n",
    "    try:\n",
    "        return html_chunk['href']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#links are in anchored tagged classes, so we will get all them with try and except\n",
    "html_chunks = soup.find_all('a')\n",
    "\n",
    "# we first now create a list of links\n",
    "crude_links = []\n",
    "for html_chunk in html_chunks:\n",
    "    crude_links.append(try_href(html_chunk))\n",
    "\n",
    "#we now remove none types\n",
    "links = [link for link in crude_links if (type(link) == str) ]\n",
    "links\n",
    "    \n",
    "#and now we add \"https://en.wikipedia.org\" to the link in case it does not have\n",
    "links = ['https://en.wikipedia.org'+link if link.startswith('/') else link for link in links]\n",
    "\n",
    "#and now we drop everything that does not starts with 'https://'\n",
    "links = [link for link in links if link.startswith('https://')]\n",
    "\n",
    "#now we print the links:\n",
    "print('the found links are:\\n')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#according to the page,  \"Titles in bold have been changed since the last release point.\"\n",
    "#also, according to the html, the changed titles have classe usctitlechanged (<div class=\"usctitlechanged\"...)\n",
    "\n",
    "html_chunks = soup.find_all('div',attrs={'class':'usctitlechanged'})\n",
    "\n",
    "#from those chunks, we need to:\n",
    "#   1 - take the text\n",
    "#   2 - strip \"\\n\"s and spaces\n",
    "#   3 - remove ' *' at the end of title if exists\n",
    "\n",
    "titles = [chunk.text.strip() for chunk in html_chunks]\n",
    "titles = [title.split(' ٭')[0] if title.endswith(' ٭') else title for title in titles]\n",
    "\n",
    "print('The changed titles are:\\n')\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the wanted dudes names is stored in html in a class title, tagged h3 (<h3 class=\"title\">)\n",
    "\n",
    "#after getting it, we extract the text, and treat it with strip:\n",
    "\n",
    "html_chunks = soup.find_all('h3',attrs={'class':'title'})\n",
    "\n",
    "names = [chunk.text.strip() for chunk in html_chunks]\n",
    "\n",
    "print('the most wanted fellows are:\\n')\n",
    "names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "\n",
    "#We first find all tables and select the fourth table, since this is the one in the html that has our info!\n",
    "table = soup.find_all('table')[3]\n",
    "\n",
    "# The head of the table (columns names) and the rows (table body) are located with tags named, respectively, \n",
    "# '<thead' and '<tbody'.\n",
    "head = table.find_all('thead')[0]\n",
    "body = table.find_all('tbody')[0]\n",
    "\n",
    "#now we look at the header of the table... The column names are located in <th class=\"th2\"...\n",
    "#we will store the text from the chunks as columns\n",
    "headers_chunks = head.find_all('th',attrs={'class':'th2'})\n",
    "columns = [headers_chunk.text for headers_chunk in headers_chunks]\n",
    "\n",
    "#now we look at the body of the table: all info is in <td>\n",
    "data = [chunk.text for chunk in body.find_all('td')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we put it into a dataframe\n",
    "nrows = int(len(data)/13)\n",
    "ncols = 13\n",
    "\n",
    "df = pd.DataFrame(np.array(data).reshape((nrows, ncols)))\n",
    "#From the df, we can see that columns 0,1,2,3 and 9 can be droped\n",
    "\n",
    "df = df.iloc[:,[3,4,5,6,7,8,10,11]]\n",
    "\n",
    "#we now will addapt our pandas dataframes with new columns, based on the columns obtained previously:\n",
    "columns = ['Date & Time UTC', 'Lat deg', 'Lat', 'Long deg', 'Long','Depth km','Mag','Region name']\n",
    "df.columns = columns\n",
    "\n",
    "#and now we treat the column 'Date & Time'\n",
    "df.iloc[:,0] = df.iloc[:,0].apply(lambda x: x.split('.')[0])\n",
    "df.iloc[:,0] = df.iloc[:,0].apply(lambda x: x.split('earthquake')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our pandas dataframe is ready!!!\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just look for \"<div class=\"central-featured\"\", and that \"anchored\" <a>\n",
    "central = soup.find_all('div',attrs={'class':'central-featured'})[0]\n",
    "crude_languages = central.find_all('a')\n",
    "\n",
    "#now separate the languages from numbers\n",
    "lang_name = [lang.find_all('strong')[0].text for lang in crude_languages]\n",
    "art_numb = [lang.find_all('bdi')[0].text for lang in crude_languages]\n",
    "\n",
    "#now generate the pandas dataframe\n",
    "wiki_langs = pd.DataFrame({'language':lang_name, 'articles number':art_numb})\n",
    "wiki_langs.sort_values(by='articles number',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all info needed is in <div class=\"grid-row dgu-topics\">\n",
    "mini_soup = soup.find_all('div',attrs={'class':\"grid-row dgu-topics\"})[0]\n",
    "mini_soup\n",
    "\n",
    "#datasets are within <h2> tags\n",
    "datasets = [element.text for element in mini_soup.find_all('h2')]\n",
    "\n",
    "#descripitions are within <p> tags\n",
    "dsets_desc = [element.text for element in mini_soup.find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally create our pandas dataframe to sumarize\n",
    "datasetes = pd.DataFrame({'Dataset':datasets,'Description':dsets_desc})\n",
    "datasetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we find all tables and select the table with the numbers (which is the first one)\n",
    "table = soup.find_all('table')[0]\n",
    "\n",
    "#we separate the body\n",
    "body = table.find_all('tbody')[0]\n",
    "\n",
    "#we extract the columns\n",
    "columns = [element.text.strip() for element in body.find_all('th')]\n",
    "\n",
    "#now we collect all row values\n",
    "data = [element.text.strip() for element in body.find_all('td')]\n",
    "\n",
    "#now we put it into a dataframe: each row has 5 elements.\n",
    "nrows = int(len(data)/5)\n",
    "ncols = 5\n",
    "\n",
    "df = pd.DataFrame(np.array(data).reshape((nrows, ncols)),columns = columns)\n",
    "\n",
    "print('showing the top 10 languages')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we find all tables and select the table with the numbers (which is the first one)\n",
    "table = soup.find_all('table')[0]\n",
    "\n",
    "#we separate the body\n",
    "head = table.find_all('thead')[0]\n",
    "\n",
    "#we separate the body\n",
    "body = table.find_all('tbody')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we extract the columns\n",
    "columns = [element.text.strip() for element in head.find_all('th')]\n",
    "\n",
    "#now we collect all row values\n",
    "data = [element.text.strip() for element in body.find_all('td')]\n",
    "\n",
    "#now we put it into a dataframe: each row has 5 elements.\n",
    "nrows = int(len(data)/5)\n",
    "ncols = 5\n",
    "\n",
    "df = pd.DataFrame(np.array(data).reshape((nrows, ncols)),columns = columns)\n",
    "\n",
    "#filtering the important columns\n",
    "df = df.iloc[:,[1,2]]\n",
    "\n",
    "#creating a year column\n",
    "df['Year'] = df.iloc[:,0].apply(lambda x: x.split('\\n')[-1])\n",
    "\n",
    "#cleaning the name of the title\n",
    "df.iloc[:,0] = df.iloc[:,0].apply(lambda x: x.split('\\n')[1])\n",
    "\n",
    "#rename columns\n",
    "df.columns =['Title', 'IMDb Rating', 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting director, etc...\n",
    "#The minisoup body.find_all('a'), only has information on odd numbers\n",
    "\n",
    "#this will give information about directors and two actors\n",
    "info_minisoup = body.find_all('a')\n",
    "\n",
    "#getting only directors\n",
    "directors = [info_minisoup[i]['title'].split(' (dir.)')[0] for i in range(len(info_minisoup)) if (i % 2 == 1)]\n",
    "\n",
    "#updating the dataframe\n",
    "df['director'] = directors\n",
    "\n",
    "#reordering the dataframe\n",
    "imdb_df = df.iloc[:,[0,2,3,1]]\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we find all tables and select the table with the numbers (which is the first one)\n",
    "table = soup.find_all('table')[0]\n",
    "\n",
    "#we separate the body\n",
    "head = table.find_all('thead')[0]\n",
    "\n",
    "#we separate the body\n",
    "body = table.find_all('tbody')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-815-ae2e8ceaa61a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#and 10 random movies:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "#we select the movies:\n",
    "movies = body.find_all('td',attrs={'class':'titleColumn'})\n",
    "\n",
    "#and 10 random movies:\n",
    "np.random(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(links[0])\n",
    "html = response.content\n",
    "subsoup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary text is in <div class=\"summary_text\">\n",
    "subsoup.find_all('div',attrs={'class':'summary_text'})[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the 10 first links:\n",
    "minisoup = soup.find_all('td',attrs={'class':'titleColumn'})\n",
    "links = ['https://www.imdb.com/' + chunck.find_all('a')[0]['href'] for chunck in minisoup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f714bf6b57084869a0d64bbc3b8b48e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#colect all summary text, but from the first 10!\n",
    "sum_text = []\n",
    "\n",
    "for link in tqdm(links[:10]):\n",
    "    response = requests.get(link)\n",
    "    html = response.content\n",
    "    subsoup = BeautifulSoup(html)\n",
    "    \n",
    "    sum_text.append(subsoup.find_all('div',attrs={'class':'summary_text'})[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sinopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The aging patriarch of an organized crime dyna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The early life and career of Vito Corleone in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A jury holdout attempts to prevent a miscarria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In German-occupied Poland during World War II,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gandalf and Aragorn lead the World of Men agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The lives of two mob hitmen, a boxer, a gangst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A bounty hunting scam joins two men in an unea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A meek Hobbit from the Shire and eight compani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sinopsis\n",
       "0  Two imprisoned men bond over a number of years...\n",
       "1  The aging patriarch of an organized crime dyna...\n",
       "2  The early life and career of Vito Corleone in ...\n",
       "3  When the menace known as the Joker wreaks havo...\n",
       "4  A jury holdout attempts to prevent a miscarria...\n",
       "5  In German-occupied Poland during World War II,...\n",
       "6  Gandalf and Aragorn lead the World of Men agai...\n",
       "7  The lives of two mob hitmen, a boxer, a gangst...\n",
       "8  A bounty hunting scam joins two men in an unea...\n",
       "9  A meek Hobbit from the Shire and eight compani..."
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sum_text,columns=['sinopsis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
