{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3YSDiy0Up_C"
   },
   "source": [
    "# Evaluation: Precision & Recall\n",
    "## Using the evaluation metrics we have learned, we are going to compare how well some different types of classifiers perform on different evaluation metrics\n",
    "### We are going to use a dataset of written numbers which we can import from sklearn. Run the code below to do so. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnUelxEmUp_D"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMgKR4YhUp_G"
   },
   "source": [
    "### Now take a look at the shapes of the X and y matricies \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prseFXcoUp_H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (70000, 784)\n",
      "y shape:  (70000,)\n"
     ]
    }
   ],
   "source": [
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRz_ivu3Up_J"
   },
   "source": [
    "### Now, let's pick one entry and see what number is written. Use indexing to pick the 36000th digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hIilw19uUp_J"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  29., 154.,\n",
       "       254., 243., 135.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  19.,  57., 166., 253., 253., 253., 254., 235.,  38.,  51.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 173., 253., 254., 253.,\n",
       "       187., 168., 169.,  93.,  41.,  48.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  76., 223., 253., 242., 116.,   6.,   0.,   0.,   0.,  98.,\n",
       "        78.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  26.,  92., 217., 254., 229., 102.,\n",
       "         0.,   0.,   0.,   0.,   7.,  19.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       150., 254., 253., 196.,   9.,   0.,   0.,   0.,   0.,   0.,  19.,\n",
       "       163., 120.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 154., 223., 254., 171.,  37.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  45., 216., 165.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51.,\n",
       "       247., 253., 229.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       157., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  13., 105., 254., 245., 176.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 105., 248., 150.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  89., 254.,\n",
       "       241., 169.,  13.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51.,\n",
       "       254., 197.,  76.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  29., 235., 254., 184.,  25.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  29., 235., 254., 134.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  98., 241.,\n",
       "       242.,  98.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26., 123.,\n",
       "       241., 242.,  98.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  83., 230., 254., 114.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  13., 157., 254., 204., 114.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  19., 188., 253.,\n",
       "       152.,  38.,   0.,   0.,   0.,   0.,   0.,   0.,  26., 170., 226.,\n",
       "       209.,  97.,  38.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0., 129., 253., 165.,  16.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  67., 223., 254., 209.,  13.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 180., 228.,\n",
       "        52.,   3.,   0.,   0.,   0.,   0.,   0., 102., 235., 253., 128.,\n",
       "        22.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  92., 254., 120.,   0.,   0.,   0.,   0.,\n",
       "        26., 205., 254., 245., 176.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 117.,\n",
       "       253., 225., 120.,  57., 132., 169., 244., 254., 215.,  81.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   7., 188., 253., 253., 254., 253.,\n",
       "       253., 215., 156.,  19.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  95., 229., 253., 255., 177.,  52.,  16.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X[35999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90UIitfWUp_L"
   },
   "source": [
    "### You can use the .reshape(28,28) function and plt.imshow() function with the parameters cmap = matplotlib.cm.binary, interpolation=\"nearest\" to make a plot of the number. Be sure to import matplotlib!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euR2jMOEUp_L"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 35999\n",
    "img = X[index].reshape(28,28)\n",
    "plt.imshow(img, cmap = cm.binary, interpolation=\"nearest\")\n",
    "plt.title(f'This letter is: {y[index]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lg12cNV4Up_O"
   },
   "source": [
    "### Use indexing to see if what the plot shows matches with the outcome of the 36000th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IIrGU5pEUp_O"
   },
   "outputs": [],
   "source": [
    "\"\"\" already done as the title of the figure!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "froaQc03Up_Q"
   },
   "source": [
    "### Now lets break into a test train split to run a classification. Instead of using sklearn, use indexing to select the first 60000 entries for the training, and the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrOasZSCUp_Q"
   },
   "outputs": [],
   "source": [
    "X_train = X[0:60000]\n",
    "y_train = y[0:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdyH3KfzUp_T"
   },
   "source": [
    "### We are going to make a two-class classifier, so lets restrict to just one number, for example 5s. Do this by defining a new y training and y testing sets for just the number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train = np.array(pd.Series(y_train).apply(lambda x: 1 if x == '5' else 0))\n",
    "y_test = np.array(pd.Series(y_test).apply(lambda x: 1 if x == '5' else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6vQQALZUp_V"
   },
   "source": [
    "### Lets train a logistic regression to predict if a number is a 5 or not (remember to use the 'just 5s' y training set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KlPavPMzUp_V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vfarn\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmXXgi5TUp_X"
   },
   "source": [
    "### Does the classifier predict correctly the 36000th digit we picked before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8MlPuH2SUp_Y"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.  16.  29. 154. 254. 243. 135.  25.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.  19.  57. 166. 253. 253. 253. 254. 235.  38.  51.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0. 173. 253. 254. 253. 187. 168. 169.  93.  41.  48.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n  76. 223. 253. 242. 116.   6.   0.   0.   0.  98.  78.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  26.  92.\n 217. 254. 229. 102.   0.   0.   0.   0.   7.  19.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 150. 254.\n 253. 196.   9.   0.   0.   0.   0.   0.  19. 163. 120.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 154. 223. 254.\n 171.  37.   0.   0.   0.   0.   0.   0.  45. 216. 165.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 247. 253. 229.\n   9.   0.   0.   0.   0.   0.   0.   0. 157. 253.  90.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.  13. 105. 254. 245. 176.   0.\n   0.   0.   0.   0.   0.   0.   0. 105. 248. 150.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.  89. 254. 241. 169.  13.   0.\n   0.   0.   0.   0.   0.   0.  51. 254. 197.  76.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.  29. 235. 254. 184.  25.   0.   0.\n   0.   0.   0.   0.   0.  29. 235. 254. 134.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.  98. 241. 242.  98.   0.   0.   0.\n   0.   0.   0.   0.  26. 123. 241. 242.  98.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.  83. 230. 254. 114.   0.   0.   0.   0.\n   0.   0.   0.  13. 157. 254. 204. 114.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.  19. 188. 253. 152.  38.   0.   0.   0.   0.\n   0.   0.  26. 170. 226. 209.  97.  38.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. 129. 253. 165.  16.   0.   0.   0.   0.   0.\n   0.  67. 223. 254. 209.  13.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. 180. 228.  52.   3.   0.   0.   0.   0.   0.\n 102. 235. 253. 128.  22.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.  92. 254. 120.   0.   0.   0.   0.  26. 205.\n 254. 245. 176.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. 117. 253. 225. 120.  57. 132. 169. 244. 254.\n 215.  81.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   7. 188. 253. 253. 254. 253. 253. 215. 156.\n  19.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.  95. 229. 253. 255. 177.  52.  16.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-15c5b2cb39b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m35999\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' y[35999] is the value: 0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The predicted value is: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vfarn\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vfarn\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vfarn\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    553\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.  16.  29. 154. 254. 243. 135.  25.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.  19.  57. 166. 253. 253. 253. 254. 235.  38.  51.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0. 173. 253. 254. 253. 187. 168. 169.  93.  41.  48.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n  76. 223. 253. 242. 116.   6.   0.   0.   0.  98.  78.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  26.  92.\n 217. 254. 229. 102.   0.   0.   0.   0.   7.  19.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 150. 254.\n 253. 196.   9.   0.   0.   0.   0.   0.  19. 163. 120.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 154. 223. 254.\n 171.  37.   0.   0.   0.   0.   0.   0.  45. 216. 165.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 247. 253. 229.\n   9.   0.   0.   0.   0.   0.   0.   0. 157. 253.  90.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.  13. 105. 254. 245. 176.   0.\n   0.   0.   0.   0.   0.   0.   0. 105. 248. 150.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.  89. 254. 241. 169.  13.   0.\n   0.   0.   0.   0.   0.   0.  51. 254. 197.  76.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.  29. 235. 254. 184.  25.   0.   0.\n   0.   0.   0.   0.   0.  29. 235. 254. 134.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.  98. 241. 242.  98.   0.   0.   0.\n   0.   0.   0.   0.  26. 123. 241. 242.  98.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.  83. 230. 254. 114.   0.   0.   0.   0.\n   0.   0.   0.  13. 157. 254. 204. 114.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.  19. 188. 253. 152.  38.   0.   0.   0.   0.\n   0.   0.  26. 170. 226. 209.  97.  38.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. 129. 253. 165.  16.   0.   0.   0.   0.   0.\n   0.  67. 223. 254. 209.  13.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. 180. 228.  52.   3.   0.   0.   0.   0.   0.\n 102. 235. 253. 128.  22.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.  92. 254. 120.   0.   0.   0.   0.  26. 205.\n 254. 245. 176.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. 117. 253. 225. 120.  57. 132. 169. 244. 254.\n 215.  81.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   7. 188. 253. 253. 254. 253. 253. 215. 156.\n  19.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.  95. 229. 253. 255. 177.  52.  16.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(pd.Series(X[35999]))\n",
    "\n",
    "# \n",
    "print(' y[35999] is the value: 0')\n",
    "print('The predicted value is: ', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALx4JWkFUp_a"
   },
   "source": [
    "### To make some comparisons, we are going to make a very dumb classifier, that never predicts 5s. Build the classifier with the code below, and call it using: never_5_clf = Never5Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7X7WdYMtUp_b"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "never_5_clf = Never5Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6b5bZIcXUp_d"
   },
   "source": [
    "### Now lets fit and predict on the testing set using our never 5 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAP7A9LlUp_e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrYZYL3aUp_g"
   },
   "source": [
    "### Let's compare this to the Logistic Regression. Examine the confusion matrix, precision, recall, and f1_scores for each. What is the probability cutoff you are using to decide the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9aPqBXOUp_g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvbi-m47Up_i"
   },
   "source": [
    "### What are the differences you see? Without knowing what each model is, what can these metrics tell you about how well each works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4JycFxNUp_i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szmx0qxBUp_k"
   },
   "source": [
    "### Now let's examine the roc_curve for each. Use the roc_curve method from sklearn.metrics to help plot the curve for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ObckzQRUp_k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPhGeQHuUp_m"
   },
   "source": [
    "### Now find the roc_auc_score for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vz4Gn4j_Up_o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUfStJ6lUp_p"
   },
   "source": [
    "### Using the yellowbrick library  plot the roc_auc_score curve for the logistic model . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTytAzX7Up_q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vda4i6JZUp_s"
   },
   "source": [
    "### What does this metric tell you? Which classifier works better with this metric in mind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94kkn4-hUp_s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
